# === develpment process of LLM ===
1. pretraining 
- trained by large, non-labeled raw text(self-supervised learning)
2. finetuning
- trained by relative smaller, labeled raw text
# === transformer architecture ===
