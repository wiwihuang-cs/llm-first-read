# === develpment process of LLM ===
1. pretraining 
- trained by large, non-labeled raw text(self-supervised learning)
2. finetuning
- trained by relative smaller, labeled raw text
# === transformer and encoder/decoder ===
- tranformer : computation rule
- encoder/decoder : components that organize the transformer blocks for different task. 
